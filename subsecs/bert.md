---
layout: default
---

[back](../index.md)

# Bidirectional encoder representations from transformers (BERT)

[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805) <br>
Authors: Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova<br>
Year: 2018

Summary:
- pre-training approach
- transformer architecture
- masked language modeling (MLM)
- next sentence prediction (NSP)
- fine-tuning
- WordPiece tokenization
- 

[back](../index.md)