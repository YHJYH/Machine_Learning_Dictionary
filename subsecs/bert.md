---
layout: default
---

[back](../index.md)

# Bidirectional encoder representations from transformers (BERT)

[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805) <br>
Authors: Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova<br>
Year: 2018

Summary:
- pre-training approach
- transformer architecture
- masked language modeling (MLM)
- next sentence prediction (NSP)
- fine-tuning
- WordPiece tokenization

## Reference
[BERT: Pre-Training of Transformers for Language Understanding](https://medium.com/swlh/bert-pre-training-of-transformers-for-language-understanding-5214fba4a9af)

[back](../index.md)